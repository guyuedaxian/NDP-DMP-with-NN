{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb0e3b38",
   "metadata": {},
   "source": [
    "## Training CNN or DNN on MNIST and Noisy MNIST dataset (n-MNIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd88f45",
   "metadata": {},
   "source": [
    "- A DNN network\n",
    "- A Simple CNN network\n",
    "- A Normal CNN network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a625a044",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba02b1e4",
   "metadata": {},
   "source": [
    "### The neuros setting of hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c0103c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuros = 500\n",
    "# neuros = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb43641",
   "metadata": {},
   "source": [
    "### 1, Deep Neural Network (DNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1d932e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNNNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DNNNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28 * 1, neuros)\n",
    "        self.fc2 = nn.Linear(neuros, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x.view(-1, 28 * 28 * 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2916b2f2",
   "metadata": {},
   "source": [
    "### 2, Simple Convolutional Neural Network(SCNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8fab7e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNNNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNNNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.fc1 = nn.Linear(12 * 12 * 20, neuros)\n",
    "        self.fc2 = nn.Linear(neuros, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "\n",
    "        x = x.view(-1, 12 * 12 * 20)\n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedab82b",
   "metadata": {},
   "source": [
    "### 3, Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "934c7d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CNNNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4 * 4 * 50, neuros)\n",
    "        self.fc2 = nn.Linear(neuros, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "\n",
    "        x = x.view(-1, 4 * 4 * 50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0430cf",
   "metadata": {},
   "source": [
    "### Training Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bc1ae6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 500 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            if False:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cffc2f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).cpu().sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.3f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    return round((100. * correct / len(test_loader.dataset)),3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69036044",
   "metadata": {},
   "source": [
    "#### 1, Training the models on MNIST datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f7e6567c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "Train a basic MNIST CNN.\n",
    "\n",
    "Based on the PyTorch example code here:\n",
    "https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import torch\n",
    "\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f1657313",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================== dnn ======================\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 503: Service Unavailable\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ../data/mnist/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "92.1%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-860cb9978964>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     train_loader = torch.utils.data.DataLoader(\n\u001b[0;32m---> 27\u001b[0;31m     datasets.MNIST('../data/mnist', train=True, download=True,\n\u001b[0m\u001b[1;32m     28\u001b[0m            transform=transforms.Compose([transforms.ToTensor(),\n\u001b[1;32m     29\u001b[0m                                          transforms.Normalize((0.1307,), (0.3081,))])),\n",
      "\u001b[0;32m~/dir_install/Aanconda3/anaconda3/envs/env_msc_project_py38/lib/python3.8/site-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, train, transform, target_transform, download)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dir_install/Aanconda3/anaconda3/envs/env_msc_project_py38/lib/python3.8/site-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    174\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Downloading {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m                     download_and_extract_archive(\n\u001b[0m\u001b[1;32m    177\u001b[0m                         \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_root\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_folder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m                         \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dir_install/Aanconda3/anaconda3/envs/env_msc_project_py38/lib/python3.8/site-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36mdownload_and_extract_archive\u001b[0;34m(url, download_root, extract_root, filename, md5, remove_finished)\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 413\u001b[0;31m     \u001b[0mdownload_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m     \u001b[0marchive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownload_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dir_install/Aanconda3/anaconda3/envs/env_msc_project_py38/lib/python3.8/site-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36mdownload_url\u001b[0;34m(url, root, filename, md5, max_redirect_hops)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Downloading '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0m_urlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mURLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'https'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dir_install/Aanconda3/anaconda3/envs/env_msc_project_py38/lib/python3.8/site-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36m_urlretrieve\u001b[0;34m(url, filename, chunk_size)\u001b[0m\n\u001b[1;32m     34\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                     \u001b[0mpbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m                     \u001b[0mfh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dir_install/Aanconda3/anaconda3/envs/env_msc_project_py38/lib/python3.8/site-packages/torch/hub.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\r{0:.1f}%\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                 \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dir_install/Aanconda3/anaconda3/envs/env_msc_project_py38/lib/python3.8/site-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mflush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    348\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m                 \u001b[0;31m# and give a timeout to avoid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m                     \u001b[0;31m# write directly to __stderr__ instead of warning because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m                     \u001b[0;31m# if this is happening sys.stderr may be the problem.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dir_install/Aanconda3/anaconda3/envs/env_msc_project_py38/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    556\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dir_install/Aanconda3/anaconda3/envs/env_msc_project_py38/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "net_list = ['dnn', 'scnn', 'cnn']\n",
    "accuracys = []\n",
    "\n",
    "learning_rate = 0.01\n",
    "step_size=1\n",
    "gamma=0.7\n",
    "momentum = 0.5\n",
    "epoch_num = 10\n",
    "seed = 1\n",
    "train_batch_size = 64\n",
    "test_batch_size = 1000\n",
    "\n",
    "for net in net_list:\n",
    "    print('=====================', net,'======================')\n",
    "    # Training settings\n",
    "    use_cuda = not False and torch.cuda.is_available()\n",
    "    accuracy_list = []\n",
    "    torch.manual_seed(seed)\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    \n",
    "    kwargs = {}\n",
    "    if use_cuda:\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        kwargs = {'num_workers': 1,'pin_memory': True}\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data/mnist', train=True, download=True,\n",
    "           transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                         transforms.Normalize((0.1307,), (0.3081,))])),\n",
    "    batch_size=train_batch_size, shuffle=True, **kwargs)\n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data/mnist', train=False,download=True,\n",
    "           transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                         transforms.Normalize((0.1307,), (0.3081,))])),\n",
    "    batch_size=test_batch_size, shuffle=True, **kwargs)\n",
    "    \n",
    "\n",
    "    if net == 'dnn':\n",
    "        model = DNNNet().to(device)\n",
    "    elif net == 'scnn':\n",
    "        model = SimpleCNNNet().to(device)\n",
    "    elif net == 'cnn':\n",
    "        model = CNNNet().to(device)\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "    for epoch in range(1, epoch_num + 1):\n",
    "        train(None, model, device, train_loader, optimizer, epoch)\n",
    "        acc = test(model, device, test_loader)\n",
    "        accuracy_list.append(acc)\n",
    "\n",
    "    if True:\n",
    "        if net == 'dnn':\n",
    "            torch.save(model.state_dict(), \"mnist_dnn_net_\"+str(neuros)+\".pt\")\n",
    "        elif net == 'scnn':\n",
    "            torch.save(model.state_dict(), \"mnist_cnn_net_simple_\"+str(neuros)+\".pt\")\n",
    "        elif net == 'cnn':\n",
    "            torch.save(model.state_dict(), \"mnist_cnn_net_\"+str(neuros)+\".pt\")\n",
    "    print(accuracy_list)\n",
    "    accuracys.append(accuracy_list)\n",
    "\n",
    "print(accuracys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f5f54c",
   "metadata": {},
   "source": [
    "#### Show the testing loss value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850c74ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d08bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(1, epoch_num+1)\n",
    "plt.figure(figsize=(12,6))\n",
    "y1 = accuracys[0]\n",
    "y2 = accuracys[1]\n",
    "y3 = accuracys[2]\n",
    "plt.plot(x, y1, label='DNN', linestyle='dotted')\n",
    "plt.plot(x, y2, label='Simple CNN', linestyle='dashdot')\n",
    "plt.plot(x, y3, label='CNN')\n",
    "ax = plt.subplot()\n",
    "for p in ax.patches:\n",
    "    ax.annotate(str(p.get_height()), (p.get_x(), p.get_height() * 1.01))\n",
    "plt.yticks(np.arange(95, 100, 0.5))\n",
    "plt.xticks(np.arange(0, 16, 1))\n",
    "plt.legend()\n",
    "plt.xlabel('Epochs', fontsize='14')\n",
    "plt.ylabel('Accuracy(%)', fontsize='14')\n",
    "# plt.title('The accuracy of the different depth neural network')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370222d3",
   "metadata": {},
   "source": [
    "#### 2,Training the models on Noisy MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2989abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import errno\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.datasets.utils import download_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9dd4d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMNIST(MNIST):\n",
    "    \"\"\"`n-MNIST <http://www.csc.lsu.edu/~saikat/n-mnist/>`_ Dataset.\n",
    "    Args:\n",
    "        root (string): Root directory of dataset where ``mnist-with-awgn.mat``,\n",
    "            ``mnist-with-motion-blur.mat`` and\n",
    "            ``mnist-with-reduced-contrast-and-awgn.mat`` exist.\n",
    "        train (bool, optional): If True, loads training data, otherwise loads\n",
    "            test data.\n",
    "        download (bool, optional): If true, downloads the dataset from the internet and\n",
    "            puts it in root directory. If dataset is already downloaded, it is not\n",
    "            downloaded again.\n",
    "        transform (callable, optional): A function/transform that  takes in an PIL image\n",
    "            and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
    "        target_transform (callable, optional): A function/transform that takes in the\n",
    "            target and transforms it.\n",
    "        dataset (string, optional): Select the particular n-MNIST dataset to load:\n",
    "            \"awgn\", \"motion-blur\", \"reduced-contrast-and-awgn\" or \"all\".\n",
    "    \"\"\"\n",
    "    urls = {\n",
    "        'awgn': 'http://www.csc.lsu.edu/~saikat/n-mnist/data/mnist-with-awgn.gz',\n",
    "        'motion-blur': 'http://www.csc.lsu.edu/~saikat/n-mnist/data/mnist-with-motion-blur.gz',\n",
    "        'reduced-contrast-and-awgn': 'http://www.csc.lsu.edu/~saikat/n-mnist/data/mnist-with-reduced-contrast-and-awgn.gz',\n",
    "    }\n",
    "\n",
    "    def __init__(self, root, train=True, transform=None, target_transform=None, download=False, dataset='awgn'):\n",
    "        self.dataset = dataset\n",
    "\n",
    "        if self.dataset != 'all':\n",
    "            self.urls = {self.dataset: self.urls[self.dataset]}\n",
    "\n",
    "        self.gzip_files = []\n",
    "        self.mat_files = []\n",
    "        for _, url in self.urls.items():\n",
    "            self.gzip_files.append(os.path.basename(url))\n",
    "            self.mat_files.append(os.path.splitext(os.path.basename(url))[0] + '.mat')\n",
    "\n",
    "        self.training_file = self.dataset + '-training.pt'\n",
    "        self.test_file = self.dataset + '-test.pt'\n",
    "\n",
    "        super(NMNIST, self).__init__(root, train=train, transform=transform, target_transform=target_transform, download=download)\n",
    "\n",
    "    def _check_gzips_exists(self):\n",
    "        for gzip_file in self.gzip_files:\n",
    "            if not os.path.exists(os.path.join(self.root, self.raw_folder, gzip_file)):\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def _check_mats_exists(self):\n",
    "        for mat_file in self.mat_files:\n",
    "            if not os.path.exists(os.path.join(self.root, self.raw_folder, mat_file)):\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def download(self):\n",
    "        \"\"\"Download the n-MNIST data if it doesn't exist in processed_folder already.\"\"\"\n",
    "        from six.moves import urllib\n",
    "        import tarfile\n",
    "\n",
    "        if self._check_exists():\n",
    "            return\n",
    "\n",
    "        # download files\n",
    "        try:\n",
    "            os.makedirs(os.path.join(self.root, self.raw_folder))\n",
    "            os.makedirs(os.path.join(self.root, self.processed_folder))\n",
    "        except OSError as e:\n",
    "            if e.errno == errno.EEXIST:\n",
    "                pass\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "        if not self._check_mats_exists():\n",
    "            for _, url in self.urls.items():\n",
    "                filename = url.rpartition('/')[2]\n",
    "                file_path = os.path.join(self.root, self.raw_folder, filename)\n",
    "                if not self._check_gzips_exists():\n",
    "                    download_url(url, root=os.path.join(self.root, self.raw_folder),\n",
    "                                 filename=filename, md5=None)\n",
    "                with open(file_path.replace('.gz', '.mat'), 'wb') as out_f:\n",
    "                    tar = tarfile.open(file_path, 'r:gz')\n",
    "                    zip_f = tar.extractfile(os.path.basename(file_path.replace('.gz', '.mat')))\n",
    "                    out_f.write(zip_f.read())\n",
    "                    os.unlink(file_path)\n",
    "\n",
    "        # process and save as torch files\n",
    "        print('Processing...')\n",
    "\n",
    "        def read_images(mat_data, split):\n",
    "            length = mat_data[split].shape[0]\n",
    "            num_rows = np.uint8(np.sqrt(mat_data[split].shape[1]))\n",
    "            num_cols = num_rows\n",
    "            return torch.from_numpy(mat_data[split]).view(length, num_rows, num_cols)\n",
    "\n",
    "        def read_labels(mat_data, split):\n",
    "            length = mat_data[split].shape[0]\n",
    "            labels = np.asarray([np.where(r == 1)[0][0] for r in mat_data[split]])\n",
    "            return torch.from_numpy(labels).view(length).long()\n",
    "\n",
    "        data = sio.loadmat(os.path.join(self.root, self.raw_folder, self.mat_files[0]))\n",
    "        if len(self.mat_files) > 1:\n",
    "            for mat_file in self.mat_files[1:]:\n",
    "                mat_data = sio.loadmat(os.path.join(self.root, self.raw_folder, mat_file))\n",
    "                data['train_x'] = np.concatenate((data['train_x'], mat_data['train_x']), axis=0)\n",
    "                data['train_y'] = np.concatenate((data['train_y'], mat_data['train_y']), axis=0)\n",
    "                data['test_x'] = np.concatenate((data['test_x'], mat_data['test_x']), axis=0)\n",
    "                data['test_y'] = np.concatenate((data['test_y'], mat_data['test_y']), axis=0)\n",
    "\n",
    "        training_set = (\n",
    "            read_images(data, 'train_x'),\n",
    "            read_labels(data, 'train_y')\n",
    "        )\n",
    "        test_set = (\n",
    "            read_images(data, 'test_x'),\n",
    "            read_labels(data, 'test_y')\n",
    "        )\n",
    "        with open(os.path.join(self.root, self.processed_folder, self.training_file), 'wb') as f:\n",
    "            torch.save(training_set, f)\n",
    "        with open(os.path.join(self.root, self.processed_folder, self.test_file), 'wb') as f:\n",
    "            torch.save(test_set, f)\n",
    "\n",
    "        print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "95f59b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/shengdaolin_sh/dir_developer/workspaces/pycharm/acs-project-msc_project_ndp/ndp/mnist_cnn\n"
     ]
    }
   ],
   "source": [
    "from os.path import dirname, realpath\n",
    "root_path = dirname(os.getcwd())\n",
    "sys.path.append(root_path)\n",
    "print(root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "714c5a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================== dnn ======================\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.305805\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.316052\n",
      "\n",
      "Test set: Average loss: 0.3519, Accuracy: 8950/10000 (89.500%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.305354\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.533873\n",
      "\n",
      "Test set: Average loss: 0.2987, Accuracy: 9109/10000 (91.090%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.338250\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.301545\n",
      "\n",
      "Test set: Average loss: 0.2576, Accuracy: 9222/10000 (92.220%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.161575\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.185538\n",
      "\n",
      "Test set: Average loss: 0.2287, Accuracy: 9313/10000 (93.130%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.270964\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.089987\n",
      "\n",
      "Test set: Average loss: 0.2074, Accuracy: 9370/10000 (93.700%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.189466\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.140427\n",
      "\n",
      "Test set: Average loss: 0.1924, Accuracy: 9427/10000 (94.270%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.088891\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.070100\n",
      "\n",
      "Test set: Average loss: 0.1790, Accuracy: 9465/10000 (94.650%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.134426\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.136964\n",
      "\n",
      "Test set: Average loss: 0.1698, Accuracy: 9474/10000 (94.740%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.083016\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.069828\n",
      "\n",
      "Test set: Average loss: 0.1640, Accuracy: 9495/10000 (94.950%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.050724\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.077853\n",
      "\n",
      "Test set: Average loss: 0.1630, Accuracy: 9494/10000 (94.940%)\n",
      "\n",
      "[89.5, 91.09, 92.22, 93.13, 93.7, 94.27, 94.65, 94.74, 94.95, 94.94]\n",
      "===================== scnn ======================\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.306627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shengdaolin_sh/dir_install/Aanconda3/anaconda3/envs/env_msc_project_py38/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ../c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.451275\n",
      "\n",
      "Test set: Average loss: 0.2278, Accuracy: 9295/10000 (92.950%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.078366\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.059162\n",
      "\n",
      "Test set: Average loss: 0.1523, Accuracy: 9513/10000 (95.130%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.088205\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.047769\n",
      "\n",
      "Test set: Average loss: 0.1410, Accuracy: 9553/10000 (95.530%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.123802\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.101916\n",
      "\n",
      "Test set: Average loss: 0.1094, Accuracy: 9656/10000 (96.560%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.131662\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.029959\n",
      "\n",
      "Test set: Average loss: 0.0843, Accuracy: 9724/10000 (97.240%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.042148\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.111388\n",
      "\n",
      "Test set: Average loss: 0.0781, Accuracy: 9759/10000 (97.590%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.035004\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.036986\n",
      "\n",
      "Test set: Average loss: 0.0735, Accuracy: 9742/10000 (97.420%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.019617\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.020462\n",
      "\n",
      "Test set: Average loss: 0.0755, Accuracy: 9744/10000 (97.440%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.071417\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.013716\n",
      "\n",
      "Test set: Average loss: 0.0628, Accuracy: 9784/10000 (97.840%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.021222\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.014755\n",
      "\n",
      "Test set: Average loss: 0.0637, Accuracy: 9786/10000 (97.860%)\n",
      "\n",
      "[92.95, 95.13, 95.53, 96.56, 97.24, 97.59, 97.42, 97.44, 97.84, 97.86]\n",
      "===================== cnn ======================\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.315960\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.198471\n",
      "\n",
      "Test set: Average loss: 0.1528, Accuracy: 9521/10000 (95.210%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.119219\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.174444\n",
      "\n",
      "Test set: Average loss: 0.0888, Accuracy: 9724/10000 (97.240%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.031630\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.061356\n",
      "\n",
      "Test set: Average loss: 0.0774, Accuracy: 9747/10000 (97.470%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.053048\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.068933\n",
      "\n",
      "Test set: Average loss: 0.0637, Accuracy: 9791/10000 (97.910%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.093242\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.113850\n",
      "\n",
      "Test set: Average loss: 0.0574, Accuracy: 9806/10000 (98.060%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.038649\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.129994\n",
      "\n",
      "Test set: Average loss: 0.0552, Accuracy: 9810/10000 (98.100%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.002954\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.047115\n",
      "\n",
      "Test set: Average loss: 0.0507, Accuracy: 9832/10000 (98.320%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.044293\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.009430\n",
      "\n",
      "Test set: Average loss: 0.0532, Accuracy: 9823/10000 (98.230%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.028281\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.002421\n",
      "\n",
      "Test set: Average loss: 0.0509, Accuracy: 9826/10000 (98.260%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.029369\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.012384\n",
      "\n",
      "Test set: Average loss: 0.0477, Accuracy: 9831/10000 (98.310%)\n",
      "\n",
      "[95.21, 97.24, 97.47, 97.91, 98.06, 98.1, 98.32, 98.23, 98.26, 98.31]\n",
      "[[89.5, 91.09, 92.22, 93.13, 93.7, 94.27, 94.65, 94.74, 94.95, 94.94], [92.95, 95.13, 95.53, 96.56, 97.24, 97.59, 97.42, 97.44, 97.84, 97.86], [95.21, 97.24, 97.47, 97.91, 98.06, 98.1, 98.32, 98.23, 98.26, 98.31]]\n"
     ]
    }
   ],
   "source": [
    "net_list = ['dnn', 'scnn', 'cnn']\n",
    "accuracys = []\n",
    "\n",
    "learning_rate = 0.01\n",
    "step_size=1\n",
    "gamma=0.7\n",
    "momentum = 0.5\n",
    "epoch_num = 10\n",
    "seed = 1\n",
    "train_batch_size = 64\n",
    "test_batch_size = 1000\n",
    "dataset = 'awgn' # awgn, motion-blur, reduced-contrast-and-awgn\n",
    "data_path = './data/n-mnist/'\n",
    "model_save_path = '../cnn_trained/'\n",
    "\n",
    "dataset = 'mnist' # \n",
    "\n",
    "if dataset == 'mnist'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for net in net_list:\n",
    "    print('=====================', net,'======================')\n",
    "    # Training settings\n",
    "    use_cuda = not False and torch.cuda.is_available()\n",
    "    accuracy_list = []\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    \n",
    "    kwargs = {}\n",
    "    if use_cuda:\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        kwargs = {'num_workers': 1,'pin_memory': True}\n",
    "        \n",
    "    # load the mnist dataset\n",
    "    if dataset = 'mnist':\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            datasets.MNIST('./data/mnist', \n",
    "                           train=True, \n",
    "                           download=True,\n",
    "                           transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,),(0.3081,))])),\n",
    "            batch_size=train_batch_size, shuffle=True, **kwargs)\n",
    "    \n",
    "        test_loader = torch.utils.data.DataLoader(\n",
    "            datasets.MNIST('./data/mnist',\n",
    "                           train=False,\n",
    "                           download=True,\n",
    "                           transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                                         transforms.Normalize((0.1307,), (0.3081,))])),\n",
    "            batch_size=test_batch_size, shuffle=True, **kwargs)\n",
    "    # load the n-mnist dataset\n",
    "    elif dataset == 'awgn' or == 'motion-blur' or =='reduced-contrast-and-awgn':\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            NMNIST('./data/n-mnist',\n",
    "                   train=True,\n",
    "                   download=True,\n",
    "                   transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                                 transforms.Normalize((0.1307,), (0.3081,))]),\n",
    "                   dataset=dataset),batch_size=train_batch_size, shuffle=True, **kwargs)\n",
    "    \n",
    "        test_loader = torch.utils.data.DataLoader(\n",
    "            NMNIST('./data/n-mnist',\n",
    "                   train=False,\n",
    "                   download=True,\n",
    "                   transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                                 transforms.Normalize((0.1307,), (0.3081,))]),\n",
    "                   dataset=dataset), batch_size=test_batch_size, shuffle=True, **kwargs)\n",
    "    \n",
    "    if net == 'dnn':\n",
    "        model = DNNNet().to(device)\n",
    "    elif net == 'scnn':\n",
    "        model = SimpleCNNNet().to(device)\n",
    "    elif net == 'cnn':\n",
    "        model = CNNNet().to(device)\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "    for epoch in range(1, epoch_num + 1):\n",
    "        train(None, model, device, train_loader, optimizer, epoch)\n",
    "        acc = test(model, device, test_loader)\n",
    "        accuracy_list.append(acc)\n",
    "\n",
    "    if True:\n",
    "        if net == 'dnn':\n",
    "            torch.save(model.state_dict(), model_save_path+'mnist_dnn_net_'+ dataset + '_' + str(neuros)+\".pt\")\n",
    "        elif net == 'scnn':\n",
    "            torch.save(model.state_dict(), model_save_path+\"mnist_cnn_net_simple_\"+ dataset + '_' + str(neuros)+\".pt\")\n",
    "        elif net == 'cnn':\n",
    "            torch.save(model.state_dict(), model_save_path+\"mnist_cnn_net_\"+ dataset+ '_' + str(neuros) +\".pt\")\n",
    "    print(accuracy_list)\n",
    "    accuracys.append(accuracy_list)\n",
    "\n",
    "print(accuracys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00edf457",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13633c62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
